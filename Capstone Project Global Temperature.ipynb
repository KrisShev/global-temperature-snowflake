{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "The aim of this project is to use data sources for global temperature and U.S. demographics and create a database where data scientists can freely investigate global temperature changes. \n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import desc,year, month, dayofmonth, weekofyear, monotonically_increasing_id\n",
    "import psycopg2\n",
    "from create_insert_snowflake_schema import drop_snowflake_schema, create_snowflake_schema, copy_snowflake_schema, copy_locations\n",
    "from create_insert_snowflake_schema import data_quality_snowflake_schema\n",
    "from create_insert_analytics_tables import drop_analytics_tables\n",
    "from create_insert_analytics_tables import create_analytics_tables\n",
    "from create_insert_analytics_tables import insert_analytics_tables\n",
    "from create_insert_analytics_tables import analytics_tables\n",
    "import os\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "The project at hand aims to create a database for users to explore global temperature changes. It has been used two particular sources: temperature csv data from Kaggle (https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data) and csv file with U.S. demographic statistics from OpenDataSoft (https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/).  The final solution is based on both datasets and represents snowflake schema that contains three dimension tables 'location', 'time', and 'us_demographics' and one fact table 'temperature'. These are used to create an exemplary OLAP cube with four tables, showcasing analytical possibilities. Spark has been used in order to clean and save data. The creation of snowflake and analtical tables has been done in Postgres.\n",
    "\n",
    "#### Describe and Gather Data \n",
    "As mentioned in the previous section, there are two types of datasets used in this project. The first one contains daily observations of temperatures with estimate uncetainty per global city. The information in this file is gather by Berkeley Earth and posted on kaggle website for free exploration. The csv file comprises of four columns - date in format 'YYYY-MM-DD', average temperature as number, average temperature uncertainty as float, city and countries as strings. \n",
    "\n",
    "The other dataset contains population statistics per city in United States. These include city and state as strings, total, female and population, number of foreign born and number of veterans as integers, median age as number and finally, race and integer counts of each race. The data is collected by U.S. Census Bureau as part of 2015 American Community Survey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_temp = '../../data2/GlobalLandTemperaturesByCity.csv'\n",
    "fname_demo = 'us-cities-demographics.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "The two datasets are investigated in this section. There are few issues identified such as data quality issues, like missing values, duplicate data.\n",
    "\n",
    "#### Cleaning Steps\n",
    "All three steps are performed below - analysing, cleaning and saving the clean data into parquet files. As this interactive process, each data issue is handled immediately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\t\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    ".enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "|        dt|AverageTemperature|AverageTemperatureUncertainty| City|Country|Latitude|Longitude|\n",
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "|1743-11-01|             6.068|           1.7369999999999999|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1743-12-01|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-01-01|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-02-01|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-03-01|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_temp = spark.read.csv(fname_temp, header=True, mode='DROPMALLFORMED')\n",
    "df_temp.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the temperature data, it is obvious right away there are missing values in the dataset. It will be addressed shortly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "|            City|        State|Median Age|Male Population|Female Population|Total Population|Number of Veterans|Foreign-born|Average Household Size|State Code|                Race|Count|\n",
      "+----------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "|   Silver Spring|     Maryland|      33.8|          40601|            41862|           82463|              1562|       30908|                   2.6|        MD|  Hispanic or Latino|25924|\n",
      "|          Quincy|Massachusetts|      41.0|          44129|            49500|           93629|              4147|       32935|                  2.39|        MA|               White|58723|\n",
      "|          Hoover|      Alabama|      38.5|          38040|            46799|           84839|              4819|        8229|                  2.58|        AL|               Asian| 4759|\n",
      "|Rancho Cucamonga|   California|      34.5|          88127|            87105|          175232|              5821|       33878|                  3.18|        CA|Black or African-...|24437|\n",
      "|          Newark|   New Jersey|      34.6|         138040|           143873|          281913|              5829|       86253|                  2.73|        NJ|               White|76402|\n",
      "+----------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_demo = spark.read.csv(fname_demo, header=True, mode='DROPMALLFORMED', sep=';')\n",
    "df_demo.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The US demographics does not show any immediate issue but will be drilled into later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dt: string (nullable = true)\n",
      " |-- AverageTemperature: string (nullable = true)\n",
      " |-- AverageTemperatureUncertainty: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Latitude: string (nullable = true)\n",
      " |-- Longitude: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_temp.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Median Age: string (nullable = true)\n",
      " |-- Male Population: string (nullable = true)\n",
      " |-- Female Population: string (nullable = true)\n",
      " |-- Total Population: string (nullable = true)\n",
      " |-- Number of Veterans: string (nullable = true)\n",
      " |-- Foreign-born: string (nullable = true)\n",
      " |-- Average Household Size: string (nullable = true)\n",
      " |-- State Code: string (nullable = true)\n",
      " |-- Race: string (nullable = true)\n",
      " |-- Count: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_demo.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both dataframes have their columns are loaded as strings. This, together the column name convention, is fixed for the temperature set first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = df_temp.withColumnRenamed('AverageTemperature', 'average_temperature') \\\n",
    "                .withColumnRenamed('AverageTemperatureUncertainty', 'average_temperature_uncertainty') \\\n",
    "                .withColumnRenamed('City', 'city') \\\n",
    "                .withColumnRenamed('Country', 'country') \\\n",
    "                .withColumnRenamed('Latitude', 'latitude') \\\n",
    "                .withColumnRenamed('Longitude', 'longitude')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = df_temp.withColumn('average_temperature', df_temp.average_temperature.cast('float')) \\\n",
    "                .withColumn('average_temperature_uncertainty', df_temp.average_temperature_uncertainty.cast('float')) \\\n",
    "                .withColumn('latitude', df_temp.latitude.cast('float')) \\\n",
    "                .withColumn('longitude', df_temp.longitude.cast('float')) \\\n",
    "                .withColumn('dt', df_temp.dt.cast('date'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dt: date (nullable = true)\n",
      " |-- average_temperature: float (nullable = true)\n",
      " |-- average_temperature_uncertainty: float (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- latitude: float (nullable = true)\n",
      " |-- longitude: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_temp.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+\n",
      "|summary|    country|\n",
      "+-------+-----------+\n",
      "|  count|    8599212|\n",
      "|   mean|       null|\n",
      "| stddev|       null|\n",
      "|    min|Afghanistan|\n",
      "|    max|   Zimbabwe|\n",
      "+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_temp.describe('country').show()\n",
    "#min and max are real countries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The min and max values for country are real country names. No values are missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+\n",
      "|       country|  count|\n",
      "+--------------+-------+\n",
      "|         India|1014906|\n",
      "|         China| 827802|\n",
      "| United States| 687289|\n",
      "|        Brazil| 475580|\n",
      "|        Russia| 461234|\n",
      "|         Japan| 358669|\n",
      "|     Indonesia| 323255|\n",
      "|       Germany| 262359|\n",
      "|United Kingdom| 220252|\n",
      "|        Mexico| 209560|\n",
      "|       Nigeria| 172347|\n",
      "|         Spain| 159594|\n",
      "|          Iran| 151651|\n",
      "|        Turkey| 150306|\n",
      "|      Pakistan| 139231|\n",
      "|         Italy| 136038|\n",
      "|   Philippines| 127700|\n",
      "|        Poland| 123082|\n",
      "|        France| 116604|\n",
      "|  South Africa|  94050|\n",
      "+--------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_temp.groupBy('country').count().orderBy(desc('count')).show()\n",
    "#no nonsense countries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further investigation into country names shows no nonsense names. Also, given that China, United States and Germany are among the countries with most number of observations. As representative of different continents, they will be analysed in more details later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n",
      "|summary|average_temperature|\n",
      "+-------+-------------------+\n",
      "|  count|            8235082|\n",
      "|   mean| 16.727432636172175|\n",
      "| stddev| 10.353442482004583|\n",
      "|    min|            -42.704|\n",
      "|    max|             39.651|\n",
      "+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_temp.describe('average_temperature').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at average temperature data, values seem to be valid with normal limit for min and max temperature across the world. However, there are missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------------------+\n",
      "|summary|average_temperature_uncertainty|\n",
      "+-------+-------------------------------+\n",
      "|  count|                        8235082|\n",
      "|   mean|             1.0285747414276099|\n",
      "| stddev|             1.1297332887072544|\n",
      "|    min|                          0.034|\n",
      "|    max|                         15.396|\n",
      "+-------+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_temp.describe('average_temperature_uncertainty').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same conclusion holds for temperature uncertainty as average temperature data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temp.where('country is null').count()/df_temp.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temp.where('city is null').count()/df_temp.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temp.where(\"country =''\").count()/df_temp.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temp.where(\"city =''\").count()/df_temp.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04234457761943769"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temp.where(\"average_temperature is null\").count()/df_temp.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04234457761943769"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temp.where(\"average_temperature_uncertainty is null\").count()/df_temp.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The six cells above confirms that the there are no null values or empty strings for cities and countries. However, 4.23% of the whole temperature population is made of null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = df_temp.dropDuplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few cleaning actions will be taken. First, removing rows with all duplicated values. Next, investigate if there are records with the same city, country and date but different temperatures. If there are, they are removed for the dataset as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+----------+-----+\n",
      "|       city|      country|        dt|count|\n",
      "+-----------+-------------+----------+-----+\n",
      "|  Rongcheng|        China|1877-02-01|    3|\n",
      "|Springfield|United States|1773-06-01|    3|\n",
      "|Springfield|United States|1788-04-01|    3|\n",
      "|  Rongcheng|        China|2007-12-01|    3|\n",
      "|Springfield|United States|1965-08-01|    3|\n",
      "|Springfield|United States|1920-09-01|    3|\n",
      "|Springfield|United States|1795-04-01|    3|\n",
      "|Springfield|United States|1995-03-01|    3|\n",
      "|  Rongcheng|        China|1920-04-01|    3|\n",
      "|Springfield|United States|1921-10-01|    3|\n",
      "|  Rongcheng|        China|1864-06-01|    3|\n",
      "|Springfield|United States|1832-05-01|    3|\n",
      "|  Rongcheng|        China|1937-03-01|    3|\n",
      "|  Rongcheng|        China|1957-01-01|    3|\n",
      "|  Rongcheng|        China|1947-04-01|    3|\n",
      "|Springfield|United States|1970-05-01|    3|\n",
      "|  Rongcheng|        China|1849-12-01|    3|\n",
      "|  Rongcheng|        China|1966-01-01|    3|\n",
      "|Springfield|United States|1808-04-01|    3|\n",
      "|Springfield|United States|1836-05-01|    3|\n",
      "+-----------+-------------+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_temp.select(['city', 'country', 'dt']).groupBy(['city', 'country', 'dt']).count().orderBy(desc('count')).show()\n",
    "#are there locations that have different measurements on the same date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-------------------------------+---------+-------+--------+---------+\n",
      "|        dt|average_temperature|average_temperature_uncertainty|     city|country|latitude|longitude|\n",
      "+----------+-------------------+-------------------------------+---------+-------+--------+---------+\n",
      "|1855-10-01|             21.515|                          2.588|Rongcheng|  China|    null|     null|\n",
      "|1855-10-01|             11.163|                          1.855|Rongcheng|  China|    null|     null|\n",
      "|1855-10-01|             16.757|                          1.717|Rongcheng|  China|    null|     null|\n",
      "+----------+-------------------+-------------------------------+---------+-------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_temp.where(\"city = 'Rongcheng' and country = 'China' and dt = '1855-10-01'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = df_temp.dropDuplicates(subset=['city', 'country', 'dt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifying removing of duplicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+----------+-----+\n",
      "|        city|      country|        dt|count|\n",
      "+------------+-------------+----------+-----+\n",
      "|      Abohar|        India|1958-10-01|    1|\n",
      "|      Akashi|        Japan|1909-05-01|    1|\n",
      "|       Aktau|   Kazakhstan|1914-12-01|    1|\n",
      "|     Aligarh|        India|1856-07-01|    1|\n",
      "|   Allahabad|        India|1937-10-01|    1|\n",
      "|   Amsterdam|  Netherlands|1878-08-01|    1|\n",
      "|       Anand|        India|1938-09-01|    1|\n",
      "|     Angarsk|       Russia|1865-09-01|    1|\n",
      "|     Antioch|United States|1980-08-01|    1|\n",
      "|     Antwerp|      Belgium|1910-03-01|    1|\n",
      "|   Apucarana|       Brazil|1859-08-01|    1|\n",
      "|      Aqtöbe|   Kazakhstan|1915-10-01|    1|\n",
      "|       Asyut|        Egypt|1852-05-01|    1|\n",
      "|      Athens|       Greece|1800-01-01|    1|\n",
      "|       Bacau|      Romania|1815-04-01|    1|\n",
      "|     Bacolod|  Philippines|1880-02-01|    1|\n",
      "|Bahia Blanca|    Argentina|2002-04-01|    1|\n",
      "|      Baiyin|        China|2000-05-01|    1|\n",
      "|        Baku|   Azerbaijan|1866-10-01|    1|\n",
      "|    Balakovo|       Russia|1913-02-01|    1|\n",
      "+------------+-------------+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_temp.select(['city', 'country', 'dt']).groupBy(['city', 'country', 'dt']).count().orderBy(desc('count')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, records with null values for any of the attributes is removed and check is performed to make sure there are no more missing values. The reason for removing null values instead of taking zeros or averages, for example, is that these numbers will enter calculations yet unknown. Thus, we cannot predict if average across week, month, year, city or country is better. This is why, the least impact on derivations later on will have deleting these records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = df_temp.dropna(subset=['city', 'country', 'dt', 'average_temperature', 'average_temperature_uncertainty'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temp.where(\"average_temperature is null\").count()/df_temp.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to spin off time dataset, columns for year, month, day and week are added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = df_temp.withColumn('year', year(df_temp.dt)) \\\n",
    "                .withColumn('month', month(df_temp.dt)) \\\n",
    "                .withColumn('day', dayofmonth(df_temp.dt)) \\\n",
    "                .withColumn('week', weekofyear(df_temp.dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dt: date (nullable = true)\n",
      " |-- average_temperature: float (nullable = true)\n",
      " |-- average_temperature_uncertainty: float (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- latitude: float (nullable = true)\n",
      " |-- longitude: float (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- week: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_temp.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turning to the U.S. demographics dataset, column names are renamed to follow easy to refer convention. Also, as all attributes are loaded as strings, the format of each column is changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demo = df_demo.withColumnRenamed('City', 'city') \\\n",
    "            .withColumnRenamed('State', 'state') \\\n",
    "            .withColumnRenamed('Median Age', 'median_age') \\\n",
    "            .withColumnRenamed('Male Population', 'male_population') \\\n",
    "            .withColumnRenamed('Female Population', 'female_population')\\\n",
    "            .withColumnRenamed('Total Population', 'total_population')\\\n",
    "            .withColumnRenamed('Number of Veterans', 'number_of_veterans')\\\n",
    "            .withColumnRenamed('Foreign-born', 'foreign_born')\\\n",
    "            .withColumnRenamed('Average Household Size', 'average_household')\\\n",
    "            .withColumnRenamed('State Code', 'state_code')\\\n",
    "            .withColumnRenamed('Race', 'race')\\\n",
    "            .withColumnRenamed('Count', 'cnt')            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demo = df_demo.withColumn('median_age', df_demo.median_age.cast('float')) \\\n",
    "                .withColumn('male_population', df_demo.male_population.cast('int')) \\\n",
    "                .withColumn('female_population', df_demo.female_population.cast('int')) \\\n",
    "                .withColumn('total_population', df_demo.total_population.cast('int')) \\\n",
    "                .withColumn('number_of_veterans', df_demo.number_of_veterans.cast('int')) \\\n",
    "                .withColumn('foreign_born', df_demo.foreign_born.cast('int')) \\\n",
    "                .withColumn('average_household', df_demo.average_household.cast('float')) \\\n",
    "                .withColumn('cnt', df_demo.cnt.cast('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- median_age: float (nullable = true)\n",
      " |-- male_population: integer (nullable = true)\n",
      " |-- female_population: integer (nullable = true)\n",
      " |-- total_population: integer (nullable = true)\n",
      " |-- number_of_veterans: integer (nullable = true)\n",
      " |-- foreign_born: integer (nullable = true)\n",
      " |-- average_household: float (nullable = true)\n",
      " |-- state_code: string (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      " |-- cnt: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_demo.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|summary|   city|\n",
      "+-------+-------+\n",
      "|  count|   2891|\n",
      "|   mean|   null|\n",
      "| stddev|   null|\n",
      "|    min|Abilene|\n",
      "|    max|   Yuma|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_demo.select('city').describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no missing values for city and the min and max represent actual city names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|summary|    state|\n",
      "+-------+---------+\n",
      "|  count|     2891|\n",
      "|   mean|     null|\n",
      "| stddev|     null|\n",
      "|    min|  Alabama|\n",
      "|    max|Wisconsin|\n",
      "+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_demo.select('state').describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same conlcusion holds for states as for cities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_demo.where('city is null').count()/df_demo.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_demo.where('state is null').count()/df_demo.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_demo.where('median_age is null').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_demo.where('male_population is null').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_demo.where('female_population is null').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_demo.where('total_population is null').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_demo.where('cnt is null').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_demo.where('state_code is null').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The eight cells above show that there are missing values only for female and male population variables.\n",
    "\n",
    "Given that race and number of people per individual race are not nessesary, these will be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demo = df_demo.drop('race', 'cnt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Duplicates of state and city are also removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demo = df_demo.dropDuplicates(subset=['city', 'state'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The null values found in the two population variables are replaced with zeros. These are not essential attributes but still interesting information to have, so they will not be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demo = df_demo.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+----------+---------------+-----------------+----------------+------------------+------------+-----------------+----------+\n",
      "|                city|         state|median_age|male_population|female_population|total_population|number_of_veterans|foreign_born|average_household|state_code|\n",
      "+--------------------+--------------+----------+---------------+-----------------+----------------+------------------+------------+-----------------+----------+\n",
      "|          Cincinnati|          Ohio|      32.7|         143654|           154883|          298537|             13699|       16896|             2.08|        OH|\n",
      "|         Kansas City|        Kansas|      33.4|          74606|            76655|          151261|              8139|       25507|             2.71|        KS|\n",
      "|           Lynchburg|      Virginia|      28.7|          38614|            41198|           79812|              4322|        4364|             2.48|        VA|\n",
      "|              Auburn|    Washington|      37.1|          36837|            39743|           76580|              5401|       14842|             2.73|        WA|\n",
      "|              Dayton|          Ohio|      32.8|          66631|            73966|          140597|              8465|        7381|             2.26|        OH|\n",
      "|Louisville/Jeffer...|      Kentucky|      37.5|         298451|           316938|          615389|             39364|       37875|             2.45|        KY|\n",
      "|              Austin|         Texas|      32.7|         475718|           456122|          931840|             37414|      181686|              2.5|        TX|\n",
      "|           Camarillo|    California|      40.8|          31941|            35682|           67623|              4384|        9735|             2.77|        CA|\n",
      "|          Pittsburgh|  Pennsylvania|      32.9|         149690|           154695|          304385|             17728|       28187|             2.13|        PA|\n",
      "|              Upland|    California|      39.7|          36226|            40221|           76447|              2898|       16552|             2.77|        CA|\n",
      "|            Columbus|          Ohio|      32.5|         413981|           435086|          849067|             40238|      101129|              2.4|        OH|\n",
      "|          High Point|North Carolina|      35.5|          51751|            58077|          109828|              5204|       16315|             2.65|        NC|\n",
      "|         New Bedford| Massachusetts|      38.6|          43793|            51166|           94959|              4185|       19024|             2.39|        MA|\n",
      "|       Newport Beach|    California|      46.8|          43161|            43970|           87131|              4847|       15318|              2.4|        CA|\n",
      "|             Shawnee|        Kansas|      40.1|          32313|            32745|           65058|              3575|        4136|             2.64|        KS|\n",
      "|          Sugar Land|         Texas|      42.1|          43889|            44240|           88129|              4377|       31169|             2.95|        TX|\n",
      "|            Appleton|     Wisconsin|      35.6|          37217|            38038|           75255|              4219|        4454|             2.49|        WI|\n",
      "|        Lehigh Acres|       Florida|      34.3|          57856|            61624|          119480|              5551|       31880|             3.55|        FL|\n",
      "|             Livonia|      Michigan|      47.4|          45369|            49264|           94633|              5397|        7175|             2.52|        MI|\n",
      "|              Newton| Massachusetts|      42.3|          41985|            46824|           88809|              1814|       21692|             2.71|        MA|\n",
      "+--------------------+--------------+----------+---------------+-----------------+----------------+------------------+------------+-----------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_demo.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the two dataframes need to be split into four tables. The location dimension table contains only distinct values for country, city, longitude and latitude. It also needs location_id which created by monotonically increasing function starting at 1. One last modification of the location table is to fill in zeros for missing latitude and longitude values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loc = df_temp.select(['country', 'city', 'longitude', 'latitude']).groupBy('country', 'city', 'longitude', 'latitude').count()['country', 'city', 'longitude', 'latitude']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loc = df_loc.withColumn('location_id', monotonically_increasing_id()+1) \\\n",
    "                .select('location_id', 'country', 'city', 'longitude', 'latitude')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loc = df_loc.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+-------+---------+--------+\n",
      "|location_id|  country|   city|longitude|latitude|\n",
      "+-----------+---------+-------+---------+--------+\n",
      "|          1|     Oman|Salalah|      0.0|     0.0|\n",
      "|          2|Argentina|Posadas|      0.0|     0.0|\n",
      "|          3|    India| Jhansi|      0.0|     0.0|\n",
      "|          4|    China| Jiutai|      0.0|     0.0|\n",
      "|          5|    China| Bozhou|      0.0|     0.0|\n",
      "+-----------+---------+-------+---------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_loc.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partition on the generic file is made by year, month and dates as the data distrbution across these expected to be not too different. Of course, the recent dates provbly have more points as cities are added to the sample but this is believed not to have a significant impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = df_temp.repartition(\"year\", \"month\", \"day\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create a dataframe with distinct values for all dates from the temperature data. Adding also columns for year, month, day and week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time = df_temp.select(['dt', 'year', 'month', 'day', 'week']).groupBy('dt', 'year', 'month', 'day', 'week').count().orderBy('dt')['dt', 'year', 'month', 'day', 'week']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-----+---+----+\n",
      "|        dt|year|month|day|week|\n",
      "+----------+----+-----+---+----+\n",
      "|1743-11-01|1743|   11|  1|  44|\n",
      "|1744-04-01|1744|    4|  1|  14|\n",
      "|1744-05-01|1744|    5|  1|  18|\n",
      "|1744-06-01|1744|    6|  1|  23|\n",
      "|1744-07-01|1744|    7|  1|  27|\n",
      "+----------+----+-----+---+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_time.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order for the end tables temperature and location to be joinable, they have to have column location_id. This is why the location dataset is joined on city and country so that the temperature dataset will identify the correct location_id. Later, the final temperature dataframe is spun off with date, average temperature, uncertainty and location_id attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loc_temp = df_loc.withColumnRenamed('country', 'country_loc')\\\n",
    "                    .withColumnRenamed('city', 'city_loc')\\\n",
    "                    .select(['location_id', 'country_loc', 'city_loc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = df_temp.join(df_loc_temp, (df_temp.city == df_loc_temp.city_loc) & (df_temp.country == df_loc_temp.country_loc), how = 'left' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-------------------------------+-------------+\n",
      "|        dt|average_temperature|average_temperature_uncertainty|  location_id|\n",
      "+----------+-------------------+-------------------------------+-------------+\n",
      "|2005-12-01|             -1.757|                          0.292|1271310319622|\n",
      "|1901-03-01|              2.071|                          0.355|1271310319622|\n",
      "|1812-09-01|             14.724|                          1.097|1271310319622|\n",
      "|1989-01-01|             -0.429|                          0.283|1271310319622|\n",
      "|1930-02-01|              0.108|                          0.205|1271310319622|\n",
      "|1921-10-01|             10.807|                          0.289|1271310319622|\n",
      "|1967-04-01|              8.633|                          0.251|1271310319622|\n",
      "|1761-11-01|              4.249|                          3.272|1271310319622|\n",
      "|1798-06-01|             21.073|                          1.807|1271310319622|\n",
      "|1958-03-01|              2.764|                          0.213|1271310319622|\n",
      "|1948-09-01|             18.135|                          0.163|1271310319622|\n",
      "|1996-06-01|             20.603|                          0.299|1271310319622|\n",
      "|1987-02-01|             -2.441|                           0.22|1271310319622|\n",
      "|1996-12-01|              2.036|                          0.216|1271310319622|\n",
      "|1946-07-01|             22.165|                          0.251|1271310319622|\n",
      "|1947-02-01|             -3.623|                          0.244|1271310319622|\n",
      "|1776-01-01|             -4.406|                          5.969|1271310319622|\n",
      "|1921-01-01|             -1.422|                          0.463|1271310319622|\n",
      "|1865-10-01|              9.368|                          1.301|1271310319622|\n",
      "|1872-01-01|              -4.27|                          1.817|1271310319622|\n",
      "+----------+-------------------+-------------------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_temp = df_temp.select('dt', 'average_temperature', 'average_temperature_uncertainty', 'location_id')\n",
    "df_temp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking up the U.S. demographics dataframe, it currently can joined to location only based on city as country is by definiton 'United States'. This is not good enough and this is why a temporary subset of the location table is made where the country is pre-filtered to be 'United States'. By joining to U.S. demographics table, the correct location_id is added. Next, a monotically increasing id as us_location_id is also added as a primary key and null location_id are set to zero. This should be a problem for the join as all id columns start at 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loc_temp_us = df_loc_temp.where(\"country_loc == 'United States'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demo = df_demo.join(df_loc_temp_us, df_demo.city==df_loc_temp_us.city_loc, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demo = df_demo.drop('country_loc', 'city_loc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demo = df_demo.withColumn('us_location_id', monotonically_increasing_id()+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demo = df_demo.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+----------+---------------+-----------------+----------------+------------------+------------+-----------------+----------+-------------+--------------+\n",
      "|                city|         state|median_age|male_population|female_population|total_population|number_of_veterans|foreign_born|average_household|state_code|  location_id|us_location_id|\n",
      "+--------------------+--------------+----------+---------------+-----------------+----------------+------------------+------------+-----------------+----------+-------------+--------------+\n",
      "|        Saint George|          Utah|      37.3|          38732|            41475|           80207|              4443|        4824|             2.81|        UT|            0|             1|\n",
      "|               Tyler|         Texas|      33.9|          50422|            53283|          103705|              4813|        8225|             2.59|        TX|            0|             2|\n",
      "|           Worcester| Massachusetts|      34.9|          90951|            93855|          184806|              9408|       36907|             2.43|        MA| 463856467978|             3|\n",
      "|              Caguas|   Puerto Rico|      40.4|          34743|            42265|           77008|                 0|           0|              0.0|        PR|            0|    8589934593|\n",
      "|          Charleston|South Carolina|      35.0|          63956|            71568|          135524|              9368|        5767|              2.4|        SC| 970662608903|    8589934594|\n",
      "|              Corona|    California|      37.1|          79749|            84493|          164242|              7709|       42131|             2.97|        CA| 386547056642|    8589934595|\n",
      "|               Pasco|    Washington|      28.6|          35298|            32799|           68097|              2052|       16265|             3.45|        WA|            0|    8589934596|\n",
      "|         Springfield|      Illinois|      38.8|          55639|            62170|          117809|              7525|        4264|             2.22|        IL| 369367187461|    8589934597|\n",
      "|         Springfield| Massachusetts|      31.8|          74744|            79592|          154336|              5723|       16226|             2.81|        MA| 369367187461|    8589934598|\n",
      "|         Springfield|      Missouri|      34.1|          81057|            85741|          166798|             12291|        7765|             2.12|        MO| 369367187461|    8589934599|\n",
      "|               Tempe|       Arizona|      28.8|          91350|            84476|          175826|              7058|       26620|             2.44|        AZ| 420906795016|    8589934600|\n",
      "|              Auburn|    Washington|      37.1|          36837|            39743|           76580|              5401|       14842|             2.73|        WA|            0|   17179869185|\n",
      "|Augusta-Richmond ...|       Georgia|      33.7|          94662|           101917|          196579|             19085|        7915|             2.67|        GA|            0|   17179869186|\n",
      "|     North Las Vegas|        Nevada|      32.2|         116350|           118443|          234793|             17256|       49815|             3.31|        NV|1297080123402|   17179869187|\n",
      "|            Palatine|      Illinois|      38.0|          36506|            32763|           69269|              2629|       21760|             2.69|        IL|            0|   17179869188|\n",
      "|            Thornton|      Colorado|      33.1|          65482|            67977|          133459|              7261|       17949|             2.97|        CO|1700807049220|   17179869189|\n",
      "|           Bethlehem|  Pennsylvania|      35.1|          35861|            38506|           74367|              4704|        6295|             2.46|        PA|            0|   25769803777|\n",
      "|             Phoenix|       Arizona|      33.8|         786833|           776168|         1563001|             72388|      300702|             2.89|        AZ|  94489280529|   25769803778|\n",
      "|           Hollywood|       Florida|      41.4|          75358|            74363|          149721|              6056|       55158|             2.65|        FL| 335007449095|   34359738369|\n",
      "|           Pittsburg|    California|      34.5|          33309|            36118|           69427|              2109|       21043|             3.15|        CA|            0|   34359738370|\n",
      "+--------------------+--------------+----------+---------------+-----------------+----------------+------------------+------------+-----------------+----------+-------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_demo.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The four cells below show the counts of the final snowflake tables. These will be used for data quality checks if the data pipeline from spark to Postgres has worked correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8190455"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temp.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3490"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_loc.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3167"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_time.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "596"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_demo.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, csv files have been created to store the four tables. The reason for using csv and parquet, for example, is discussed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp.write.csv('temperature.csv', header='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loc.write.csv('location.csv', header='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time.write.csv('time.csv', header='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demo.write.csv('us_demographics.csv', header='true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "\n",
    "The aim of the project is to create a playground for data scientists to investigate trends for global temperature changes. With this in mind, the conceptual data model chosen for this project is a snowflake schema. It represents three dimension tables and one fact table. The dimension tables are Location, Time and US_demographics. Two of them describe particular feature of the fact table (Location and Time tables) and one of them describes further another of the dimension tables. That is US_demograpics has further information about Location table.  \n",
    "\n",
    "The reason for choosing snowflake schema as a data model is that it is intuitve and easy to understand by users as compared to normalized data model. Also, it has been decided to choose snowflake and not snowflake schema because the nature of the us_demographics table is that brings extra information only for very small subset of the Location table. If these attributes are added to the Location table directly, it will result in much bigger table with a lot of null values. The size on disc would increase without adding any actual benefit. The U.S. subset will be used only for specific purposes such as which city has the greatest number of citizens to target for a marketing campaign about global warming. Its use is very limited and thus better be spun off.\n",
    "\n",
    "The data model is being presented graphically below.\n",
    "\n",
    "As an example of possible analysis based on the proposed data model, four analytic tables have been made as well. This mini-OLAP cube is not considered part of the data model per se but just as a show-case of analytics that can be build on topo of the snowflake schema.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkQAAAE/CAYAAACn7dzFAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAADsMAAA7DAcdvqGQAACQxSURBVHhe7d09jhzLmYXhuwAZMrQAAdyAnAHGlEdXhjC2XHp3BwLGI2RoCTJpDbQEGVyAVkBoCVpCDw4vj3Tup8jq6qzI7i8Y7wMEuip/IiKT8XPUM7z84QkAAGBzBCIAALA9AhEAANgegQgAAGyPQAQAALZHIAIAANsjEAEAgO0RiAAAwPYIRAAAYHsEIgAAsD0CEQAA2B6BCAAAbI9ABAAAtkcgAgAA2yMQAQCA7RGIAADA9ghEAABgewQiAACwPQIRAADYHoEIAABsj0AEAAC2RyACAADbIxABAIDtEYgAAK/u3bt3Tz/88MPTly9fvh35OZ/PMrq2XlO5Hv1M9b4snz59enr//v3Xz58/f/52x9PX9uu1td6j9vS9HkMvBCIAwKtzcKghJ0NH8jEHFP3U9wwZOqYgY67LbWW4MdeT90kNRApJ9Trfq2L+rqJ7jEDUH4EIAPDqjgLRhw8f/iNMSA0kNbCMfPz48V/X6Kfqru4NRM/1V22J69LPDEAEov4IRACAV3cUMI6O+7c9PufPt6iuGqCqewKR2xsFmhrU/NlhzEFJ947uRx8Eohs8MUdFg9wDvhZNHk8SldEkyHv9v1pG9eX/ovEEzeK6j/pSefKr6B47ul9FbRwtGn7OW8+gYtl+lvq/BgF837y+1uDjNaEezxB0K6CY1xqvLV6b6lpztLZ5vdV5XzNqz2ug78/r9FlFdOxWf/H2CER3OJp8/lXpaDP35NM9+qkJlXTME85hotbnOnzeddWFQuq9RxM4+1XPWS4E5vrqouHF4OgZaj/q9QD2dLSe6djoeK7DR2ty8lrkenxPXcOO1rZcB+s6luqaltf5nNZdHbvVX7w9AtEdjiZf3fyTg0cNNZKTJM/V+mq7+qnvnuBp1Bd9V0mqQxPdbWuiV7kQ2NGi4Wc5egbRdxWp1wPY09F65vUn1xCpa8et9VB07qjkPUdrW10HR/dKXfP02Wu2uJ/6mcfRD4HoDpoAdZDLaPO3DESeWKbvqssT0RO81lfb1U99Hy0A9d7RJPcxXeO63XaqC4GM6hPVlfU81496PYA9Ha1nXjPqeqtjeb3XkrxO57TW+JzW35Trsh2tbXUd9L25do36Wr/7mnoc/RCI7uDwUAezN/8snmg58Tw59dN16bgnylGY8HfXqfb1PYsn662+mK8x11fVhUDc17po+NnqM2TJfvj6LLVOAN8/rzNZvPZ5ncwyChMZNlzEdTs82WgtP1rbXEeug6P1y2uf6Vjtq+saPQP6IBDd4blA5EmcFAJ0zmFAnzUpfFx1eiLeGybUvo7VSS61L74/r9X3nPTuS+3/aCE4WjS8QNRnOOpHvR4AgA4IRHeYEYgcMlQcKo4C0ag+eUkgqsHD30elhpxRIDp6B67Xz3lvP/wdAIAOCER3mBGIHATy2JWBSPRdRRxy6r2jOkeBSHw82/D99lw//B78zAAAdEAgusNRIHLoqUXX10AkeV5eGogcSLK4T6N7fcw/dX816udRIBLXlSXd6ofa8DPXcvTMAAC8BgIRACzmx/97T6G0K6sjEAHAYr6HzQffFwIRAODVEYjQDYFoEr1ICqVbAbpifKKb72FMtglEQCeMSXTG+EQ3BKJJmNzohjGJzhif6IZANAmTG90wJtEZ4xPdEIgmYXKjG8YkOmN8ohsC0SRMbnTDmERnjE90QyCahMmNbhiT6IzxiW4IRJMwudENYxKdMT7RDYFoEiY3umFMojPGJ7ohEE3C5EY3jEl0xvhENwSiSZjc6IYxic4Yn+iGQDQJkxvdMCbRGeMT3RCIJmFyoxvGJDpjfKIbAtEkTG50w5hEZ4xPdEMgmoTJjW4Yk+iM8YluCESTMLnRDWMSnTE+0Q2BaBImN7phTKIzxie6IRBNwuRGN4xJdMb4RDcEokle8iLfv3//9MMPP/ysfPr06dvZ53348OFrOePLly//0fa7d+++nf2Jvud59TeN+p9G5z9+/Pj13Kjvoz75etH19Zg88h52wIaDzhif6IZANMlLA1Fu7g4E927ujwQBt6WfI6N+qL8Zmmr/df2t86n2XUFQbdZAqPo+f/789bPrr/2udeHn2HD2ovmxUvnv//n5/xAD3hqBaJJHApE4qDgE+LuLA4PuzeMZCI6Op1uBSH1S/SO6J/uQ/Vefdd5eEoiy3iO+R/XmvbUu/ByBaC85B1fA+EQ3BKJJHg1Eot+C+Hgubv4tio2CQN4r+j4KGrcC0a0gk2GkXqfvKnarnux7fa4jvsfBK39z5Lrwn9hw9nLPXOqE8YluCESTzAhER8driKlBoP6GRlRPhhRzXVl83VGIkmxT1+f9tc/1vIplPWpLbZrqyXtGwUc/3d88jv/EhrOXnGcrYHyiGwLRJDMCUf6Wx789yfJcIKrFwSHVcJXqb5lS9jk/u59Z39HzSfb91m+IdPzoN0E6p3vrcfycxqTeVcfyl7/85VsvMYve60oIROiGQDTJo4HIQUUhwAHHIaOGmBoEfP09al1JdY5ClLhvUvuv79mfewOR+6JwU2V7eY+oboW3ehw/13Vy/+EPfyAQXeDeNaALAhG6IRBN8mgg0mLmzb0GHF2r7w4x+l6Dy63f7qRbgcjnaj2qO9ur/a+/6bk3EImu0701FOnYUSAS9UnX1OP4NwLRXnIOroBAhG4IRJO8NBBp8cpSA4E2ep/zZ4cYBxeVDB4+NjpntwKR1Xpq6BgFngxko+dzHfpZ63MAzKI67NY9o2fETwhEe9F8WAmBCN0QiCZhcqMbAtFeCETAYwhEkzC50Q2BaC8EIuAxBKJJmNzohkC0FwIR8BgC0SRMbnRDINoLgQi3/kILnkcgmoTJjW4IRHvZORApBORfxLhS/VvAor/0ob9YcrXn2rkViPw3c13yfflvCmfJv+ijz9mu7tU1R22pn0fndXz0DL4ni//yj87VP1/VPfudE4gmIRChGwLRXrSBrOR7CkRdPBeIMuTou4KG1MDjZ9RPGQUih6LKf5NZ512/qR4dV12u23RtXq/ncP2uM+/R93yeGQhEkxCI0A2BaC+jzamz1wpE2jT1blxqYNDmPDrnDV/Fdde6VKS270DhUjd6fc/6793YazvZHx1Xqc9nes5sR58dcvKz5fX1vNupdYqfrwYc0X26fnSuHqv/iRid87O77dkIRJMQiNANgWgv2jxW8hqBSJtvbqreZL2J657chL3J6lhu9HmPw07K9n2+/jZD14h+Zn3u4z2ynfpbE9fjdqoaXvTZz5ufxXX5vdXz6oPayf6Y+3Qr4IzeYb1en7NNcd3+ORuBaBICEbohEO2lbjDdvUYg8sadvPHmBv2crGe0mWf7dWOXPD/q6739eK4efdfxkRqI1KavdQByqUHkKBCJrnc40XXuU30Ptb+1P7o2+1DfoagOnavPPQuBaBICEbohEO1FG8VKXisQ5aYrvtaB6Ig3ZhcHgHsCka+1DBSjvqq+e37jkffqcw0NOlfbNrU/eh6pgee589mO+uB+ZMjJ45LnROeyzrze73j0To6Oz0AgmoRAhG4IRHvRRrGS1wpENSB44731GyJt1Hlf1nNPIPLGbnk+P9u9m/xz9ei7jo/UQJJq4NH3fMZ6Ptvx+6j35HvwNaPi567vTZ/r80neMxuBaBICEbohEO1FG8VKXiMQeZN26HEI8oaqTT43YW/6GR58jwPAKEhl+978c9PWd9c36mu9/sitdnRO393PKp+p0vEMPJJ9rufVh2xH52rbGXCOwo3uy2v8Wfx8+Z4ln3k2AtEkBCJ0QyDaizaKlcwORHr+LN6c67m6meY5BwBvxi7aqHOz1+au4w4JOpcbvurJ+12v1GtF19yzydd789kcKLKfSX3NfqQaeET1+Fg9rz5kO37eDC8ZcI6ez/2XvN7UTj12VNcMBKJJCETohkC0F28sq2DNRDcEokmY3OiGQLQXAhHwGALRJExudEMg2guBaH36P0vpz3FU6v/pCPMRiCZhcqMbAtFetGmuhDUT3RCIJmFyoxsC0V4IRMBjCESTMLnRDYFoLwQi4DEEokmY3OiGQLQXAhHwGALRJExudEMg2guBCHgMgWgSJvfLaQG/6j+wBQLRbghEwGMIRJO85EXW/8rnldTO6L+K+hrt66+Q3vqrokeBSMd0LsvoGbKM/gun9Zo0Ou93orpqfaM+5X/11fXV/xKsjr/Wn3VFINqLxt9KCETohkA0yUqBqAst4LcC0Uj9d4VM4Sufs75jBZxb/+n5VAORrhv1Vcf8n6pXff5viKRb7VyNQLSXoznTFYEI3RCIJpkViHROC5tL/tswDgMuuUHncdetTT2Pq27Rxp2/yfCG71J/8+F/p8Yl+3RLfc7sj9t8aSCqYcX8blxfbbvWWc+nbKPWe8T16d1mvbfauRqBaC9Hc6YrAhG6IRBNMiMQ6biK6Zpc5PTZYUUbta/VJuyQ4o3f31VH1ikZiGobvj+DRdanoFDrO6Lr/JwOVaZ6sp3kPozUMJcyjGTbou/Z73o+ZSDSNar3Oa7Pz+n3daudqxGI9nI0Z7oiEKEbAtEkjwYibaC5kZpDgzbaezZm8T2idjIISIaKUcDI/tW+qt57+3GrHsl+JgeiLA4oR/dIbS/vr23X8ypWA5GuNR3Pe0bBR599fx5/bQSiveQYXgGBCN0QiCaZFYgqBxaV3JjTKEA4NNQNXWogqgEjA0Htq9u6R96rzzV4ZT/TrTZ0vNZj+VzZto7pvgyb9blSPr+uGQXAGmCzPp/Tc9xq52qvMbn/+c9/Pv3jH/94Ufn973//9Kc//Wl47u9///vT3/72t2nlr3/969fwNav8+c9/fvrjH/84rfz4449fA+KMojH3u9/97um3v/3ttPKb3/zm6de//vW0ovdnBCJ0QyCaZFYgyk1bvLFqU7+1MWewyO9qR+2lGohqwMj+1b7eCivVrXqk9ttutaF6HFZSfX+1vXrfqD+Wgch9qf18rj3dr2O32rnaaExqE1a/Z5Vf/vKXw43vVvnFL37x9Ktf/Wp4ThvwaGM+WxQQRuHhbFGAqaHmkaKAMApeZ4r+PBQAR8HwbFFAHQXXs0UB2ghE6IZANMmjgUi8gZqucQjyBuzwou+6tm7M/m2IN/BRkMoQpDZ0vTkAHG30t8JKlffWdnQu+5luteFzNcTp2K3A4/di9XzKQCT6XPta3/uoPp1XOWrnal0nt4KFNnDMleN7BQQidEMgmuSlgcibpYs3W4UVH6tBxmHAxZuxw4aK6tZ9uXn7nDf5DETiDd/F9Urd6N2He9R787kd1LKf9lwbDiNZauiobYva87Hsi4vfj376szlQja6XUXu+J9/1ayIQ7UVjbSUEInRDIJqEyY1uCER7IRABjyEQTcLkRjcEor0QiIDHEIgm2W1yj/7PVlne6v9MhH8jEO1F824lBCJ0QyCahMmNbghEeyEQAY8hEE3C5EY3BKK9EIiAxxCIJmFyoxsC0V4IRMBjCESTMLnRDYFoLwQi4DEEokmY3OiGQLQXAhHwGALRJExudEMg2guBCHgMgWgSJje6IRDthUAEPIZANAmTG90QiPZCIAIeQyCahMmNbghEeyEQAY8hEE3C5EY3BKK9EIiAxxCIJmFyoxsC0V4IRMBjCESTMLnRDYFoLwQi4DEEokmY3OiGQLQXAhHwGALRJExudEMg2guBCHgMgWgSJje6IRDthUAEPIZANAmTG90QiPZCIAIeQyCahMmNbghEeyEQAY8hEE3C5EY3BKK9EIiAxxCIJmFyoxsC0V4IRMBjCESTMLnRDYFoLwQi4DEEokmY3OiGQLQXAhHwGALRJExudEMg2guBCHgMgWgSJje6IRDthUAEPIZANAmTG90QiPayYiCiULqV1RGIgIGuY5JAdI3VAhGA+QhEwACBaC8EIgBtAhGF0q10RCC6BoEIAKsAsBAC0TUIRABYBYCFEIiuQSACwCoALIRAdA0CEQBWAWAhBKJrEIgAsAoACyEQXYNABIBVAFgIgegaBCIArALAQghE1yAQAWAVABZCILoGgQgAqwCwEALRNQhEAFgFgIUQiK5BIALAKgAshEB0DQIRAFYBYCEEomsQiACwCgALIRBdg0AEgFUAWAiB6BoEIgCsAsBCCETXIBABYBUAFkIgugaBCACrALAQAtE1CEQAWAWAhRCIrkEgAsAqACyEQHQNAhEAVgFgIQSiaxCIALAKAAshEF2DQASAVQBYCIHoGgQiAKwCwEIIRNcgEAFgFQAWQiC6BoEIAKsAsBAC0TUIRABYBYCFEIiuQSACwCoALIRAdA0CEQBWAWAhBKJrEIgAsAoACyEQXYNABIBVAFgIgegaBCIArALAQghE1yAQAWAVABZCILoGgQgAqwCwEALRNQhEAFgFgIUQiK5BIALAKgAshEB0DQIRAFYBYCEEomsQiACwCgALIRBdg0AEgFUAWAiB6BoEIgCsAsBCCETXIBABYBUAFkIgugaBCACrALAQAtE1CEQAWAWAhRCIrkEgAsAqACyEQHQNAhEAVgFgIQSiaxCIALAKAAshEF2DQASAVQBYCIHoGgQiAKwCwEIIRNcgEAFgFQAWQiC6BoEIAKsAsBAC0TUIRABYBYCFEIiuQSACwCoALIRAdA0CEQBWAWAhBKJrEIgAsAoACyEQXYNABIBVAFgIgegaBCIArALAQghE1yAQ3Ufvqb6rL1++fD327t27b0eenj58+PCva1V0zXNcTy0fP378dsVP1E695v3791/PZbv6XOk6n//06dO3o09Pnz9//tdxF9dZ1etuPXc9b2rb57Mfkudcsi9+BvXZ3H9fp3em78+9O/+51Hc/enc7YBVoZjShXDTYPVk8YHPyjCaeJ0bek8ey3LNo4W0RiK6h8Y/nea1I3ky9/tTNWOePwkWqm7p4fcu1Td9VRhwWvPGnrF8/HUTc3wwBo3ZHx0T3+17X7bDie+rz63r3sZ6r76/WUduQ5wJRBh7TMT9LPaf+Zf27GI8qtOAJk0HFk6OGG19bB7GOeQL5Hv3Ud9WFtRCIrqH5gOfpPdV3VQPRaMO+R93Ura5X+qwy4ra9LuYa53rqOX1WqVzXPe1affb6bkzH1B/3Kdf4GmZE31Vk9H7ru6t13Frzj977jm7/6eJNvSQQ+aePi6+t525NDvRGILqG5gOep/dU31Xd9L3eqHhDvsfRxlzXPNc9kmGh1uU63D/VeysM5LpZ+3Ak25fRfT6ma9x+vie3m8f0XUVqG1Kfo9bh+3MvST6vsjNWgcZeGog8UUzfVYcni+/RT31XXVgLgegaOW9wTO+pvqsaiMRrjEtu7kfqpm51/cp6XbxGZlhwH3QuQ4jXTB3z8dqm+DrV4+v8HP7u4r65/Sy6NnldNl2T392u28p+yKgNFz9HrcPnj/jPMMuOWAUae2kg8jn99ADX8aNAlMUTB70RiK6hOYDneb1IXmtyU7cMDlqHbvE6VcNJXfNc34jDgupyfVrbMoR4zVS9rru2Kb5OP2sfrB7P9jNkmI9lPV6P/X7cbpa8PtswP6ufI/suruce3ndG7+R7xyrQ2EsDkXgg+7ju9WTxPZ6AqgtrIRBdQ/MBzxutSd7kjzbQuuEfqZu61fVKn1VGdG+25WtVvEZ6bVR97rtKlXX5uhr6bgUiqeuzv4+K66j3VLUNqe+u1jG658hzf57fM1aBxs4EIg/8HNCeLL6nLjBYB4HoGpoPeJ7XG68l4vXEm63Wrdx4de6e91s3dfF6l8du1ef1z+27b3nMz+D1z9d4DZVRu75v1D+/j9p+DVJe0ysd83G3k/1JtQ2p767W4fMqSf3RM4ye86j979nzoxRv5kwg8vk85snge7wA6FqshUB0Dc0H3CfXGJfR5pzlHg4PtdSN2etiFm/oNSy4Lw4k4jUz1z8fy1LblaM+uq5RWPExr7teh1Ouye7LqH0ZteHn9HsY1THqu/cW1+ly1Pb3jlWgsTOBSDyofZ8nS72nlmwHPRGIrqHxD2BvrALAQghE1yAQAWAVABZCILoGgeh6/j8LHRXgrTEKgYUQiK7xPWzIP/7fewqlXVkJgQhYCIHoGt9LIAI6IRABuAyB6BoEImA+AtEJemkUSrfSEYHoGgQiYL7VxmSbQAR0QiDaC4EImI9AdAITGd0QiPZCIALmIxCdwERGNwSivRCIgPkIRCcwkdENgWgvBCJgPgLRCUxkdEMg2guBCJiPQHQCExndEIj2QiDCI/TvQ+76D6LeQiA6gYmMbghEe9k5EOmf1Kibub77H4OW/Gc37tn4/S+rX0n/0HX20f+I9VX03P7X5CudG70Xv4cs5v665L/ILzrmf3B79C/c29E59Wd0vP4TKu63+5r/yLfqHj3XvQhEJxCI0A2BaC/aCFZ3VSDShpqb9VEoSG8RiN6S3tcoONT3oGv8/up71bPk82Q40T0q9Xl1XnWo1OCjY7VO0ffsq9rxvXm9jj36Z0ggOoFAhG4IRHu5evN+DVcFIgWP3LjvcSsQqS6dU3Eb5uMqatf1uDhM6Fwe97XZT2/oLhkY9F3PODp3pAYw9cX3j96h1PegdtzH/Cyq388nuk/3i477mX1M1K6Oq558Bn3WPbV9qX113eLr9TOPn0UgOoFAhG4IRHupm8aKrgpEos1R72i06Y+MNmJRPVmHNnJ/v2cDzo1f12Yf1aYDRm7sonvyez6L6lHbz8n2dG/ec/Ru6nvQ/b6vBqL6brK/fjf1z8p110CU1/ley3P1PYnOq2TfziIQnUAgQjcEor3kprWqKwORaXOtG+hIDQJWj2XIOPoz0Hmdc7knENVzor773mwr77sl66whQ++rvkPxe3DJdhzSXOr9Oub37PYyROWfkY5lIMp7dZ/ut/o+fZ25z/l8ZxGITiAQoRsC0V60Aazu7JgdhZ/RMdPx0eafvKlW9ViGjNH1Op8hIkNN3itq09fWc/KWgWgkw42v00/L79me7tFn/fR5fc73ontr8bX556efGZYs63sEgegEAhG6IRDtRRvG6s6OWW18uWGKvntD1KaZm7835FtU1+idavPN4JB1jerVd2/YrjM3/tzMdV51+HM+k5/R8nPed4vacyCqQUL15XOZ+zGiPmW7qjtDXPZfbfnd6KfOZfuqx+8lA4/pWh+r51WX77Ws7xEEohMIROiGQLSXo01rJY+MWW+yLt58Lc/lpn3EQSCLN/c8lhtzvcd90ObsY9rYc6POa3V/Boz6TG5f9N3qfUdUXz579kvnHg1Evtb9zM8ZiMRtmurxe8n7TNe6rRqI9DnDlWR9jyAQnUAgQjcEor0cbVorYR1FNwSiE5jI6IZAtBcC0cvlb0iy3PMbpK5Gz6My+u0PnkcgOoFAhG4IRHvRprc61lF0QyA6gYmMbghEeyEQAfMRiE5gIqMbAtFeCETAfASiE5jI6IZAtBcCETAfgegEJjK6IRDtRYFo9fL7//2vb08D9EAgOoFAhG4IRFgN6yi6IRCdwERGNwQirIZ1FN0QiE5gIqMbAhFWwzqKbghEJzCR0Y3G5Oj/T6NDIRBhhHUU3RCITmAioxvGJFbDmEU3BKITmMjohjGJ1TBm0Q2B6AQmMrphTGI1jFl0QyA6gYmMbhiTWA1jFt0QiE5gIqMbxiRWw5hFNwSiE5jI6IYxidUwZtENgegEJjK6YUxiNYxZdEMgOoGJjG4Yk1gNYxbdEIhO6P7SRv9xvC9fvnw7+7x37949ffr06du3l9F9te3Pnz9/O/tT3XlO31O2rT77uqP+H53/+PHj1+P1ObJOlw8fPvzsXPZXdOzs+3gtbC5YDWN2Pq17KjiHQHTCCoEoN3WHlHs39UcDUYYct+3AUut+//7912KjQKTzDi1JE1/n6vOK6tE9Wbe4TvdH9N2LSL1Hx/N5umJzwWpeY8x6vl9J61WuT1qLrmzT696IznktS+5TlroGZsn1tN5b18M8p+L29bOu216vXepeoGPZL90/ep6rEIhOWC0QiYOJ+buLB2EeU/GArZMiB3LS8TphMuTkZ6nX53kvZvqun5Wudb/qBNY5358TbHRMk84T1+ddnz5nf7siEGE132sgeku3AlGus0nvJ++pa2q9N9dL0bW5nloNRKqjvqesV4FIJa/RZwLRMQLRHXIwW270Kvm/MDTo8rsGaYaAGhL8fTQJdF+deFlfrVvf66Tx+Wyn3pft1OdVfa5Tz5UTqvbd37NuXe/78r10RiDCat46EGn90DmVXIPEx1W0NrgeF68LOpfHfa3XJqnXeO0Rfdc643N13R5Rfdlf9cX36/goQKje7JOprtEal2tovbfeo3bzmUz9cB1H7SSd97vyezh6nqsQiE5YMRDJ0XENwhzw+qxjNgoG+j4aqLUuXaN2Tef03SXbkWzbi5B+1no9eUTX5HPl93qf68wymsw+N3pfHRGIsJq3DER1/dIa4e+5thzR9bnGeOMXtek1R9dk+7o2v+uz29W5us6OZHu6N+/J+pL74eL+HQWO7IvuzTW0vrusV8V0jft5T7Dxe9d1bvue+2YiEJ2wYiDywuDjGmg5iHPA67MGpmlA5rUuo4Gq++p1Ket2vSnPu88OLPqs/ntym4+L7s1ncR1HdWriefKlnJQrIBBhNW8ZiOoxrQ/evEfXS10zc83xvaI2vQZpHanrpM55/cm28r5bsj2HCBu1J+rrqG7VM7pedXr983rrUq/XMT9P0nXuZ7ZT67N8FvVV1x317yoEohNWDEQaaB58GmA5OXQuv+tznWT3hoNaV1Xr1vcc8Hnei5knmyaH+qGfnmiSz6vz+l6L+1/r9Pf6vl7yzB0QiLCalQJRXde0NnjNyHtFbfparSO5vonOef3JtvK+W7I99cPrpYzaE/V1VPfROqf6XU/e6/fp/kv9brrf/Ry147osn8XvO/vxGghEJ6wWiPRZxzzY6uDU55ws+p6D0AM36zzigXxE5+oEzuvzfJ18fo48Ju7bUT9Vn++pdYqeNxc0GU3gzghEWM1bBqK6xuW6U9co0XevB3WdyXOi817TvGaZrs31Ls/lfbeojqOgofryuUz9GNXtZ8nn1Wcd0zmp96rtXC/z2qR+1Ovyu9s2PUf2Q991fvQ8VyEQnbBCIKqlDlgNcJ/TIM0B7wmh4knviZ3F55LuvTWpdS4HvaguH8vznjDZd02SXADEfamLQ9I1Oj+q08+Wx27V1RGBCKt5zUCUxfM8j+WmW+/JtcnHtDbk+pfX6v5cA1W3z6uk/F7vO6I2Mlhkv3Qun8XU16O6R+8o1Xt9vX5Kvdd9Uz+yn5J9Vck1Vp/9rsXr8uh5rkIgOoHNB90wJrEaxiy6IRCdwERGN4xJrIYxeyx/i5LlNX9bsiMC0QlMZHTDmMRqGLPohkB0AhMZ3TAmsRrGLLohEJ3AREY3jEmshjGLbghEJzCR0Q1jEqthzKIbAtEJTGR0w5jEahiz6IZAdAITGd0wJrEaxiy6IRCdwERGN4xJrIYxi24IRCcwkdENYxKrYcyiGwLRCUxkdMOYxGoYs+iGQHQCExndMCaxGsYsuiEQncBERjeMSayGMYtuCEQnMJHRDWMSq2HMohsC0QlMZHTDmMRqGLPohkB0AhMZ3TAmsRrGLLohEJ3AREY3jEmshjGLbghEJzCR0Q1jEqthzKIbAtEJTGR0w5jEahiz6IZAdAITGd0wJrEaxiy6IRCdwERGN4xJrIYxi24IRCcwkdENYxKrYcyiGwLRCUxkdMOYxGoYs+iGQHQCExndMCaxGsYsuiEQncBERjeMSayGMYtuCEQnMJHRDWMSq2HMohsC0QlMZHTDmMRqGLPohkB0AhMZ3TAmsRrGLLohEJ3AREY3jEmshjGLbghEJzCR0Q1jEqthzKIbAtEJTGR0w5jEahiz6IZAdAITGd0wJrEaxiy6IRCdwERGN4xJrIYxi24IRCcwkdENYxKrYcyiGwLRCUxkdMOYxGoYs+iGQHQCExndMCaxGsYsuiEQncBERjeMSayGMYtuCEQnMJHRDWMSq2HMohsC0QlMZHTDmMRqGLPohkB0AhMZ3TAmsRrGLLohEJ3AREY3jEmshjGLbghEJzCR0Q1jEqvRmKVQupWVEIiAAcYkAOyFQAQMMCYBYC9tAhGF0q0AAPbRIhABAAC8JQIRAADYHoEIAABsj0AEAAC2RyACAADbIxABAIDtEYgAAMD2CEQAAGB7BCIAALA9AhEAANgegQgAAGyPQAQAALZHIAIAANsjEAEAgO0RiAAAwPYIRAAAYHsEIgAAsD0CEQAA2B6BCAAAbI9ABAAAtkcgAgAA2yMQAQCA7RGIAADA9ghEAABgewQiAACwPQIRAADYHoEIAABsj0AEAAC2RyACAADbIxABAIDtEYgAAMD2CEQAAGB7BCIAALA9AhEAANgegQgAAGzu6en/AVXs3OWxXRjwAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(filename='global_warming_data_model.PNG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "\n",
    "Outline of all pipelines:\n",
    "\n",
    "* Open csv datasets in Spark\n",
    "                |\n",
    "                v\n",
    "* Wrangler data by removing duplicates, clean missing values in Spark\n",
    "                |\n",
    "                v\n",
    "* Save four tables into csv files in Spark\n",
    "                |\n",
    "                v\n",
    "* Drop if any of the four snowflake tables exist in Postgres\n",
    "                |\n",
    "                v\n",
    "* Create if any of the four snowflake tables does not exist in Postgres\n",
    "                |\n",
    "                v\n",
    "* Copy data from the 4 csv files into the snowflake tables in Postgres\n",
    "                |\n",
    "                v\n",
    "* Drop if any of the four analytics tables exist in Postgres\n",
    "                |\n",
    "                v\n",
    "* Create if any of the four analytics tables does not exist in Postgres\n",
    "                |\n",
    "                v\n",
    "* Insert data into the analytics tables from pre-specified select statements in Postgres\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Here are built the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_queries(cur, conn, input_queries, file='', partitioned=False):\n",
    "    \"\"\" Runs over list of queries and executes them on a given connection. \n",
    "    If partitioned is set to True, input_queries is expected to be dictionary \n",
    "    with keys as table name over which queries taken from input_query values are run\"\"\"\n",
    "    if partitioned:\n",
    "        for location in input_queries.keys():\n",
    "            folder = copy_locations[location]\n",
    "            for file in os.listdir('/home/workspace/{}'.format(folder)):\n",
    "                if file[0] != '.' and file[0] != '_':\n",
    "                    cur.execute(input_queries[location].format(folder, file))\n",
    "    else:\n",
    "        for query in input_queries:\n",
    "            cur.execute(query)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a connection to Postgres server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn =psycopg2.connect(\"host=127.0.0.1 dbname=studentdb user=student password=student\")\n",
    "cur = conn.cursor()\n",
    "conn.set_session(autocommit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run puplines for creating snowflake schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_queries(cur, conn, drop_snowflake_schema)\n",
    "\n",
    "execute_queries(cur, conn, create_snowflake_schema)\n",
    "\n",
    "execute_queries(cur, conn, copy_snowflake_schema, partitioned=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run puplines for creating analytics schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_queries(cur, conn, drop_analytics_tables)\n",
    "    \n",
    "execute_queries(cur, conn, create_analytics_tables)\n",
    "    \n",
    "execute_queries(cur, conn, insert_analytics_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Two specific data quality checks are performed to ensure the pipeline ran as expected. \n",
    " * Integrity constraints on the relational database - each of the snowflake tables have at least one not null contraint. Also, the Location table has primary key for location_id that makes each location entered in the table unique. The Time table has date as primary key which makes every date entered unique. Lastly, US_demographics has us_location_id as primary key so no duplicates of U.S. cities are entered.\n",
    "\n",
    "* Source/Count checks to ensure completeness. There are two types of checks - completeness by counts for the snowflake and records display - for the analytics schema. For the snowflake schema as part of the cleaning process in Spark, all counts of all the four dataframes are noted down. These are use to reconcile with the Postgres tables. For the analytics tables, first 10 rows are retrieved so to ensure data is streaming in.\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts=\"\"\"select count(*) as cnt \n",
    "from {}\"\"\"\n",
    "\n",
    "for table in data_quality_snowflake_schema.keys():\n",
    "    cur.execute(counts.format(table))\n",
    "    row = cur.fetchone()\n",
    "    while row:\n",
    "        if row[0] != data_quality_snowflake_schema[table]:\n",
    "            raise ValueError (\"Number of observations for {}  expected: {}, found: {}\".format(table, data_quality_snowflake_schema[table], row[0]))\n",
    "        row = cur.fetchone()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 rows for table yearly_average_temperature_city\n",
      "(1820, 'Abadan', 'Iran', 32.877)\n",
      "(1820, 'Khorramshahr', 'Iran', 32.877)\n",
      "(1861, 'Guna', 'India', 32.333)\n",
      "(1861, 'Lalitpur', 'India', 32.333)\n",
      "(1861, 'Jhansi', 'India', 32.333)\n",
      "(2013, 'Jibuti', 'Djibouti', 31.082625)\n",
      "(1861, 'Ratlam', 'India', 30.813)\n",
      "(1861, 'Godhra', 'India', 30.813)\n",
      "(1861, 'Vejalpur', 'India', 30.813)\n",
      "(2010, 'Umm Durman', 'Sudan', 30.7301666666667)\n",
      "Top 10 rows for table yearly_average_temperature_country\n",
      "(2013, 'Djibouti', 31.082625)\n",
      "(2012, 'Djibouti', 30.2654166666667)\n",
      "(2000, 'Djibouti', 30.2156666666667)\n",
      "(2009, 'Djibouti', 30.2060833333333)\n",
      "(2010, 'Djibouti', 30.187)\n",
      "(2005, 'Djibouti', 30.1635833333333)\n",
      "(2011, 'Djibouti', 30.1505)\n",
      "(2007, 'Djibouti', 30.1368333333333)\n",
      "(1998, 'Djibouti', 30.1325833333333)\n",
      "(2006, 'Djibouti', 30.127)\n",
      "Top 10 rows for table monthly_average_temperature_year\n",
      "(1846, 7, 24.6283062374245)\n",
      "(2010, 7, 24.5117724928367)\n",
      "(2006, 7, 24.4012676217766)\n",
      "(2002, 7, 24.315122922636)\n",
      "(2012, 7, 24.217023495702)\n",
      "(2010, 8, 24.1573074498566)\n",
      "(2012, 8, 24.1546727793696)\n",
      "(1853, 7, 24.1349851042702)\n",
      "(1847, 7, 24.1338304140127)\n",
      "(2013, 7, 24.1280203438396)\n",
      "Top 10 rows for table yearly_average_temperature_us_population\n",
      "(1839, 'Henderson', 'Nevada', 285658, 146246, 139412, 28.008)\n",
      "(1839, 'Las Vegas', 'Nevada', 623769, 313201, 310568, 28.008)\n",
      "(1839, 'North Las Vegas', 'Nevada', 234793, 118443, 116350, 28.008)\n",
      "(1839, 'Paradise', 'Nevada', 236946, 114162, 122784, 28.008)\n",
      "(1839, 'Spring Valley', 'Nevada', 193955, 97147, 96808, 28.008)\n",
      "(1839, 'Sunrise Manor', 'Nevada', 192608, 99103, 93505, 28.008)\n",
      "(1848, 'Henderson', 'Nevada', 285658, 146246, 139412, 27.9406666666667)\n",
      "(1848, 'Las Vegas', 'Nevada', 623769, 313201, 310568, 27.9406666666667)\n",
      "(1848, 'North Las Vegas', 'Nevada', 234793, 118443, 116350, 27.9406666666667)\n",
      "(1848, 'Paradise', 'Nevada', 236946, 114162, 122784, 27.9406666666667)\n"
     ]
    }
   ],
   "source": [
    "top_10 = \"\"\" select * from {}\n",
    "                limit 10\"\"\"\n",
    "\n",
    "for table in analytics_tables:\n",
    "    cur.execute(top_10.format(table))\n",
    "    print('Top 10 rows for table {}'.format(table))\n",
    "    row = cur.fetchone()\n",
    "    while row:\n",
    "        print(row)\n",
    "        row=cur.fetchone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Close postgres connection\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary \n",
    "Data dictionary for the snowflake data model is created in data_dictionaty.txt. For each field, a brief description explanation is porvided of what the data is and where it came from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Rationale for the choice of tools and technologies for the project.\n",
    "\n",
    "There are few notes to add as why certain technologies have been used instead of others. \n",
    "\n",
    "First, Spark is used for initial analysis and clean up because of its intuitiveness to work with when cleaning data plus its capabilities of dealing with big datasets by partitioning into smaller ones. \n",
    "\n",
    "Second, Postgres is used instead of Apache Cassandra for example because 1) file distribution and partition is already being created by using Spark and 2) the assumption of the project is that queries used by data users are yet unknown. By giving them access to the snowflake schema, they will analyse in depth temperature data and build an understanding of their future needs. Afterwards, the data model can be build in Apache Cassandra.\n",
    "\n",
    "Third, partitioned files for the four clean tables are being saved in csv and not the industry standard parquet format. This is due to the decision to use Postgres and the copy command. According to <https://www.postgresql.org/docs/current/sql-copy.html> the file formats that work with that function are text, csv and binary.\n",
    "\n",
    "* How often the data should be updated and why.\n",
    "Given that the natural unit of time for the temperature data is day, it makes sense to update the temperature dataset at the most daily. The potential period should not exceed a month though as the number of days mutiples with the number of cities and this can create rather high number of total observations to be appended.  \n",
    "\n",
    "When it comes to U.S. census data, it can be updated on yearly as soon as new national statistics are published.\n",
    "\n",
    "* How you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " \n",
    " If data increased by 100 times the parallilized function in Spark would be used so to handled the huge amount of data\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " \n",
    " If the data is needed in a timely matter, it may be considered using Apache Airflow for scheduling purposes and creating SLAs to deliver data on time.\n",
    " * The database needed to be accessed by 100+ people.\n",
    " \n",
    " Given that the connection limit to Postgres is 100, in this situation it should be discussed switching to Cassandra as its current limit is 128. It is not a substantial improvement but it is still  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
